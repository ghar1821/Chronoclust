"""
Evaluator script to provide evaluation of chronoclust run.
It requires a xml file to find the location of the files produced by ChronoClust.
"""

import argparse
import csv
import textwrap
import json
import pandas as pd

from accuracy_measure import get_accuracy_precision_recall
from entropy_measure import get_entropy_purity
from tracking_evaluator import evaluate_tracking
from unique_cluster_number_evaluator import evaluate_unique_clusters
from noise_evaluator import calculate_noise


parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter,
                                 description=textwrap.dedent('''\
                                 Clustering evaluator
                                 -----------------------
                                 This script evaluate the quality of clustering by calculating:
                                 1) Entropy
                                 2) F-score
                                 3) Tracking accuracy
                                 4) Number of unique clusters
                                 
                                 It requires a config file formatted as JSON.

                                 The config file must contain all the elements listed below:
                                 {
                                    "CLUSTER_POINTS_DIR": "/project/cytoclust...",
                                    "OUTPUT_DIR": "/project/cytoclust...",
                                    "DAYS": [0,1,2,..],
                                    "TRANSITION_RULE_FILE": "/project/cytoclust/.../rule.csv",
                                    "SYNTHETIC_DATASET": "true",
                                    "CLUSTER_RESULT_FILE": "/project/../result.csv"
                                 }
                                 CLUSTER_POINTS_DIR: directory containing the files outlining the cluster points. 
                                 OUTPUT_DIR: output Directory.
                                 DAYS: list of days in the clustering.
                                 TRANSITION_RULE_FILE: file containing the transition rule. Can be empty string if you 
                                    do not wish to evaluate the tracking.
                                 SYNTHETIC_DATASET: is the clustering for synthetic dataset? - we have different way of
                                    evaluating unique number of clusters for synthetic dataset.
                                 CLUSTER_RESULT_FILE: result file of clustering generated by chronoclust
                                 
                                 ''')
                                 )
parser.add_argument('config', nargs='?', help='Location of the config file (in JSON format) for this script.')
args = parser.parse_args()

# These MUST to be assigned from config json file.
cluster_points = None
output_dir = None
transition_rule_files = None
days = None
synthetic_dataset = None
cluster_result_file = None
output_overall_file = ""


def prepare_output_files():
    with open(output_overall_file, 'w') as f:
        writer = csv.writer(f)
        writer.writerow(
            ['Day', 'Overall_Entropy', 'Overall_Purity', 'Overall_Accuracy', 'Overall_Precision', 'Overall_Recall',
             'Overall_Fscore', 'Overall_Support'])


def parse_config_file():
    """
    Before running just about any function here, you will need to parse the config file first.
    So run this before running any functions!
    This is because it sets global variables required to do any labelling.
    """

    # Need this as it defines the global variable
    global entropy_details, precision_details, output_overall_file, cluster_points, \
        output_dir, transition_rule_files, days, synthetic_dataset, cluster_result_file

    # Parse json config file
    with open(args.config, 'r') as f:
        config = json.load(f)

        cluster_points = config['CLUSTER_POINTS']

        output_dir = config['OUTPUT_DIR']
        output_overall_file = '{}/cluster_quality_overall.csv'.format(output_dir)

        transition_rule_files = config['TRANSITION_RULE_FILE']
        days = config['DAYS']
        synthetic_dataset = bool(config['SYNTHETIC_DATASET'] == 'True')
        cluster_result_file = config['CLUSTER_RESULT_FILE']


def evaluate(cluster_point_file):
    # No race condition as we're just accessing.
    # MUST make sure number of days is same as number of cluster point files.
    idx = cluster_points.index(cluster_point_file)
    day = days[idx]

    overall_entropy, overall_purity = get_entropy_purity(cluster_point_file)
    averages, accuracy = get_accuracy_precision_recall(cluster_point_file)

    with open(output_overall_file, 'a') as f:
        writer = csv.writer(f)
        writer.writerow([day, "%.2f" % overall_entropy, "%.2f" % overall_purity, "%.2f" % accuracy,
                         averages[0], averages[1], averages[2], averages[3]])


def compute_noise_proportion():
    """
    Compute the noise proportion using noise evaluator, then write the csv out

    """

    result = []
    for d, c in zip(days, cluster_points):
        true_noise_found, true_noise_not_found = calculate_noise(c)
        result.append([d, true_noise_found, true_noise_not_found])

    df = pd.DataFrame(result, columns=['Day','Noise_clustered_as_noise','Noise_not_clustered_as_noise'])
    df.to_csv("{}/noise_proportions.csv".format(output_dir), index=False)


parse_config_file()
prepare_output_files()

# pool = mp.Pool()
# pool.map(evaluate, cluster_points)
for c in cluster_points:
    evaluate(c)

# Evaluate the tracking quality
if transition_rule_files:
    evaluate_tracking(transition_rule_files, cluster_result_file, output_dir)

# Evaluate number of unique clusters
if synthetic_dataset:
    evaluate_unique_clusters(cluster_result_file, output_dir, normal_dataset=False)
else:
    evaluate_unique_clusters(cluster_result_file, output_dir)

# Compute noise frequency
compute_noise_proportion()

